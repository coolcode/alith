[package]
name = "alith-inference"
description = "Alith inference package"
version.workspace = true
edition.workspace = true
homepage.workspace = true
license.workspace = true
readme.workspace = true
repository.workspace = true

[dependencies]
alith-core.workspace = true
alith-models.workspace = true

tokio.workspace = true
thiserror.workspace = true
anyhow.workspace = true
async-trait.workspace = true
serde.workspace = true
serde_json.workspace = true

# Server

reqwest.workspace = true
tokio-util = { version = "0.7", features = ["codec", "net"] }
tokio-graceful = "0.2.2"
tokio-stream = { version = "0.1.15", default-features = false, features = [
  "sync",
] }
hyper = { version = "1.0", features = ["full"] }
hyper-util = { version = "0.1", features = ["server-auto", "client-legacy"] }
futures-util = "0.3.29"
http = "1.1.0"
http-body-util = "0.1"

# ONNX Runtime
ort = { "git" = "https://github.com/pykeio/ort.git", default-features = false, features = [
  "ndarray",
  "fetch-models",
], optional = true }

# mistralrs
indexmap = { version = "2.9" }
mistralrs = { git = "https://github.com/EricLBuehler/mistral.rs.git", rev = "aaafc2ef", optional = true }

# vLLM

# sglang

# Python
pythonize = { version = "0.23", optional = true }
either = "1.15.0"
async-stream = "0.3.6"
encoding_rs = "0.8.35"
rand = "0.9.1"
bytes = "1.10.1"
chrono = "0.4.41"

[target.'cfg(not(windows))'.dependencies]
# llamacpp
llama-cpp-2 = { version = "0.1.102", optional = true }

[features]
ort = ["dep:ort"]
llamacpp = ["dep:llama-cpp-2"]
mistralrs = ["dep:mistralrs"]
sglang = []
trt = ["ort/tensorrt"]
trtllm = []
vllm = []
python = ["dep:pythonize"]
wasm = []

# Need to install CUDA toolkit for developping including nvcc, cudnn, cublas, etc.
# Commnent this line to prevent lint errors.
# cuda = ["ort/cuda", "mistralrs/cuda", "llama-cpp-2/cuda"]
# metal = ["mistralrs/metal", "llama-cpp-2/metal"]
# vulkan = ["llama-cpp-2/vulkan"]
cuda = ["ort/cuda"]
metal = []
vulkan = []
